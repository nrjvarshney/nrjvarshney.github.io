<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Neeraj Varshney | Publications</title>
  <meta name="description" content="Personal website of Neeraj Varshney">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="../assets/css/all.min.css">
  <link rel="stylesheet" href="../assets/css/academicons.min.css">
  <link rel="stylesheet" href="../assets/css/main.css">
  <link rel="canonical" href="../publications/index.html">
  <link rel="shortcut icon" type="image/x-icon" href="../assets/img/logo.png" />
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="../index.html"><span class="font-weight-bold">Neeraj</span> Varshney</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="../index.html">
              About
              
            </a>
          </li>
          
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    CV
                    
                  </a>
              </li>
            
          
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="../projects/index.html">
                    Articles
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="../publications/index.html">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <!-- <li class="nav-item ">
                  <a class="nav-link" href="../resources/index.html">
                    Resources
                    
                  </a>
              </li>
             -->
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
<!--   <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress> -->

  <!-- Content -->
  <div class="content">
    
  <!-- <h1>Publications</h1>
  <h6></h6>
 -->



<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          AAAI
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Can Open-Domain QA Reader Utilize External Knowledge Efficiently like Humans?</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://luomancs.github.io/", target="_blank">Man Luo</a>, and <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            AAAI'23 Workshop on Knowledge Augmented Methods for NLP
        
      </p>
      <p  class="periodical font">
        <!-- <i>Do all instances need inference through the big models for a correct prediction?</i><br> 
        Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on 'model cascading', a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible. -->

      </div>
    
      <div class="col p-0">        
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#odqa-abstract" role="button" aria-expanded="false" aria-controls="odqa-abstract">Abstract</a>         -->
        
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#cascading-bibtex" role="button" aria-expanded="false" aria-controls="odqa-bibtex">BibTeX</a> -->                 
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://knowledge-nlp.github.io/aaai2023/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2211.12707" target="_blank">Paper</a>                

      </div>          
      <!-- 
      <div class="col mt-2 p-0">
        <div id="odqa-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <i>Do all instances need inference through the big models for a correct prediction?</i><br> 
        Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on 'model cascading', a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="varshney2022investigating-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{varshney-etal-2022-investigating,
    title = "Investigating Selective Prediction Approaches Across Several Tasks in {IID}, {OOD}, and Adversarial Settings",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.158",
    pages = "1995--2002",
    abstract = "In order to equip NLP systems with {`}selective prediction{'} capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.",
}</pre>

          </div>
        </div>
      </div> -->
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          EMNLP
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr> and <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            Conference on Empirical Methods in Natural Language Processing
        
      </p>
      <p  class="periodical font">
        <!-- <i>Do all instances need inference through the big models for a correct prediction?</i><br> 
        Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on 'model cascading', a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible. -->

      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#cascading-abstract" role="button" aria-expanded="false" aria-controls="cascading-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#cascading-bibtex" role="button" aria-expanded="false" aria-controls="cascading-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://2022.emnlp.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2210.05528" target="_blank">Paper</a>                

      </div>          
      
      <div class="col mt-2 p-0">
        <div id="cascading-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <i>Do all instances need inference through the big models for a correct prediction?</i><br> 
        Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on 'model cascading', a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="cascading-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@article{varshney2022model,
  title={Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems},
  author={Varshney, Neeraj and Baral, Chitta},
  journal={arXiv preprint arXiv:2210.05528},
  year={2022}
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          EMNLP
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks</h5>
      <div class="author">
        <a href="https://homes.cs.washington.edu/~yizhongw" target="_blank">Yizhong Wang</a>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, ..., <nobr><em>Neeraj Varshney</em></nobr>, ..., <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>, <a href="https://homes.cs.washington.edu/~yejin/" target="_blank">Yejin Choi</a>, <a href="https://homes.cs.washington.edu/~hannaneh/" target="_blank">Hannaneh Hajishirzi</a>, <a href="https://nasmith.github.io/" target="_blank">Noah A. Smith</a>, <a href="https://danielkhashabi.com/" target="_blank">Daniel Khashabi</a>
          
      </div>
      <div>
        <p class="periodical font-italic">
          
            Conference on Empirical Methods in Natural Language Processing
        
      </p>
      <!-- <p  class="periodical font">Prior work has shown that existing 'selective prediction' techniques fail to perform well, especially in the out-of-domain setting. In this work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instances. Using these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model's prediction. We instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81%, 5.64%) and (6.19%, 13.9%) over MaxProb --a selective prediction baseline-- on NLI and DD tasks respectively.
      </div> -->
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#wang2022benchmarking-abstract" role="button" aria-expanded="false" aria-controls="wang2022benchmarking-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#wang2022benchmarking-bibtex" role="button" aria-expanded="false" aria-controls="wang2022benchmarking-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://2022.emnlp.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2204.07705" target="_blank">Paper</a>                
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="wang2022benchmarking-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce Natural-Instructions v2, a collection of 1,600+ diverse language tasks and their expert written instructions. More importantly, the benchmark covers 70+ distinct task types, such as tagging, in-filling, and rewriting. This benchmark is collected with contributions of NLP practitioners in the community and through an iterative peer review process to ensure their quality. This benchmark enables large-scale evaluation of cross-task generalization of the models -- training on a subset of tasks and evaluating on the remaining unseen ones. For instance, we are able to rigorously quantify generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances, and model sizes. As a by-product of these experiments. we introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark. We hope this benchmark facilitates future progress toward more general-purpose language understanding models.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="wang2022benchmarking-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
          @article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>



<!-- -------- -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          ACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Unsupervised Natural Language Inference Using PHL Triplet Generation</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://pratyay-banerjee.github.io/" target="_blank">Pratyay Banerjee</a>, <a href="https://tejas-gokhale.github.io/" target="_blank">Tejas Gokhale</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            Findings of Association for Computational Linguistics
        
      </p>
      <p  class="periodical font">We explore three unsupervised settings for NLI and propose a procedural data generation approach that outperforms the existing approaches by ~13% and raises the state-of-the-art unsupervised performance on SNLI to 66.75%.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2021unsupervised-abstract" role="button" aria-expanded="false" aria-controls="varshney2021unsupervised-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2021unsupervised-bibtex" role="button" aria-expanded="false" aria-controls="varshney2021unsupervised-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.2022.aclweb.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.findings-acl.159/" target="_blank">Paper</a>      

          <a class="badge grey waves-effect font-weight-light mr-1" href="./Posters/UnsupervisedNLI.pdf" target="_blank">Poster</a>          
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="varshney2021unsupervised-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning. As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data. Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines. Furthermore, fine-tuning our model with as little as ~0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances. Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="varshney2021unsupervised-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{varshney-etal-2022-unsupervised,
    title = "Unsupervised Natural Language Inference Using {PHL} Triplet Generation",
    author = "Varshney, Neeraj  and
      Banerjee, Pratyay  and
      Gokhale, Tejas  and
      Baral, Chitta",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.159",
    pages = "2003--2016",
    abstract = "Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning. As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data. Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75{\%}, 65.9{\%}, 65.39{\%} in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines. Furthermore, fine-tuning our model with as little as {\textasciitilde}0.1{\%} of the human-annotated training dataset (500 instances) leads to 12.2{\%} higher accuracy than the model trained from scratch on the same 500 instances. Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.",
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->



<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          ACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            Findings of Association for Computational Linguistics
        
      </p>
      <p  class="periodical font">Selective Prediciton enables systems to abstain from making predictions when they are likely to be incorrect. In this work, we systematically study 'selective prediction' in a large-scale setup of 17 datasets across several NLP tasks. We conduct experiments in in-domain, out-of-domain, and adversarial settings and evaluate several selective prediction approaches such as MaxProb, Monte-Carlo Dropout, Label Smoothing, and Calibration (C, R, and T). Our investigation results in numerous interesting findings.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2022investigating-abstract" role="button" aria-expanded="false" aria-controls="varshney2022investigating-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2022investigating-bibtex" role="button" aria-expanded="false" aria-controls="varshney2022investigating-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.2022.aclweb.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.findings-acl.158/" target="_blank">Paper</a>                

          <a class="badge grey waves-effect font-weight-light mr-1" href="./Posters/Investigating_SP.pdf" target="_blank">Poster</a>
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="varshney2022investigating-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In order to equip NLP systems with selective prediction capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline 'MaxProb' remains to be explored. To this end, we systematically study 'selective prediction' in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="varshney2022investigating-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{varshney-etal-2022-investigating,
    title = "Investigating Selective Prediction Approaches Across Several Tasks in {IID}, {OOD}, and Adversarial Settings",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.158",
    pages = "1995--2002",
    abstract = "In order to equip NLP systems with {`}selective prediction{'} capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.",
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          ACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">ILDAE: Instance-Level Difficulty Analysis of Evaluation Data</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            Association for Computational Linguistics
        
      </p>
      <p  class="periodical font">We conduct <b>I</b>nstance-<b>L</b>evel <b>D</b>ifficulty <b>A</b>nalysis of <b>E</b>valuation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications such as efficient evaluations, improving quality of evaluation datasets, dataset analysis to guide future data creation, etc.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2022ildae-abstract" role="button" aria-expanded="false" aria-controls="varshney2022ildae-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2022ildae-bibtex" role="button" aria-expanded="false" aria-controls="varshney2022ildae-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.2022.aclweb.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.acl-long.240/" target="_blank">Paper</a>                

          <a class="badge grey waves-effect font-weight-light mr-1" href="./Posters/ILDAE.pdf" target="_blank">Poster</a>                
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="varshney2022ildae-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Knowledge of questions' difficulty level helps a teacher in several ways, such as estimating students' potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in NLP? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications result in several interesting findings, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our analyses and findings will bring more attention to this important yet understudied field of leveraging instance difficulty in evaluations.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="varshney2022ildae-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{varshney-etal-2022-ildae,
    title = "{ILDAE}: Instance-Level Difficulty Analysis of Evaluation Data",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.240",
    pages = "3412--3425",
    abstract = "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students{'} potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5{\%} instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2{\%} higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- --------- -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          ACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">NumGLUE: A Suite of Mathematical Reasoning Tasks</h5>
      <div class="author">
        <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://scholar.google.com/citations?user=ItxA4esAAAAJ&hl=en" target="_blank">Arindam Mitra</a>, <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.com/citations?user=d0sZa-oAAAAJ&hl=en" target="_blank">Bhavdeep Singh Sachdeva</a>, <a href="https://allenai.org/team/peterc" target="_blank">Peter Clark</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>, <a href="http://ashwinkalyan.com/" target="_blank">Ashwin Kalyan</a>
          
      </div>
      <div>
        <p class="periodical font-italic">
          
            Association for Computational Linguistics
        
      </p>
      <p  class="periodical font">We proposed a multi-task benchmark that evaluates AI systems on eight different numerical understanding tasks and showed that it is far from being solved with neural models including large language models performing significantly worse than humans (lower by 46.4%).Proposed a knowledge-retrieval based MTL method that outperforms existing models.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mishra2022numglue-abstract" role="button" aria-expanded="false" aria-controls="mishra2022numglue-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mishra2022numglue-bibtex" role="button" aria-expanded="false" aria-controls="mishra2022numglue-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.2022.aclweb.org/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.acl-long.246/" target="_blank">Paper</a>   


      </div>          
      
      <div class="col mt-2 p-0">
        <div id="mishra2022numglue-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4% on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="mishra2022numglue-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{mishra-etal-2022-numglue,
    title = "{N}um{GLUE}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
    author = "Mishra, Swaroop  and
      Mitra, Arindam  and
      Varshney, Neeraj  and
      Sachdeva, Bhavdeep  and
      Clark, Peter  and
      Baral, Chitta  and
      Kalyan, Ashwin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.246",
    pages = "3505--3523",
    abstract = "Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 {\%}). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 {\%} on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          ACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Towards Improving Selective Prediction Ability of NLP Systems</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            Repl4NLP @ Association for Computational Linguistics
            
      </p>
      <p  class="periodical font">Prior work has shown that existing 'selective prediction' techniques fail to perform well, especially in the out-of-domain setting. In this work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instances. Using these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model's prediction. We instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81%, 5.64%) and (6.19%, 13.9%) over MaxProb --a selective prediction baseline-- on NLI and DD tasks respectively.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2020s-abstract" role="button" aria-expanded="false" aria-controls="varshney2020s-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#varshney2020s-bibtex" role="button" aria-expanded="false" aria-controls="varshney2020s-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/repl4nlp2022/home?authuser=0" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.repl4nlp-1.23/" target="_blank">Paper</a>                

          <a class="badge grey waves-effect font-weight-light mr-1" href="./Posters/SP.pdf" target="_blank">Poster</a>
          
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="varshney2020s-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            It's better to say "I can't answer" than to answer incorrectly. This selective prediction ability is crucial for NLP systems to be reliably deployed in real-world applications. Prior work has shown that existing selective prediction techniques fail to perform well, especially in the out-of-domain setting. In this work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instances. Using these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model's prediction. We instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81%, 5.64%) and (6.19%, 13.9%) over 'MaxProb' -- a selective prediction baseline -- on NLI and DD tasks respectively.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="varshney2020s-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
@inproceedings{varshney-etal-2022-towards,
    title = "Towards Improving Selective Prediction Ability of {NLP} Systems",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    booktitle = "Proceedings of the 7th Workshop on Representation Learning for NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.repl4nlp-1.23",
    pages = "221--226",
    abstract = "It{'}s better to say {``}I can{'}t answer{''} than to answer incorrectly. This selective prediction ability is crucial for NLP systems to be reliably deployed in real-world applications. Prior work has shown that existing selective prediction techniques fail to perform well, especially in the out-of-domain setting. In this work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instances. Using these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model{'}s prediction. We instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81{\%}, 5.64{\%}) and (6.19{\%}, 13.9{\%}) over {`}MaxProb{'} -a selective prediction baseline- on NLI and DD tasks respectively.",
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          NAACL
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Let the Model Decide its Curriculum for Multitask Learning</h5>
      <div class="author">
          <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
      </div>
      <div>
        <p class="periodical font-italic">
          
            DeepLo @ North American Chapter of the Association for Computational Linguistics
            
        
      </p>
      <p  class="periodical font">We propose two classes of techniques to arrange training instances into a learning curriculum based on difficulty scores computed via model-based approaches. The two classes i.e Dataset-level and Instance-level differ in granularity of arrangement. Through comprehensive experiments with 12 datasets, we show that instance-level and dataset-level techniques result in strong representations as they  lead to an average performance improvement of 4.17% and 3.15% over their respective baselines. Furthermore, we find that most of this improvement comes from correctly answering the difficult instances, implying a greater efficacy of our techniques on difficult tasks.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtl-abstract" role="button" aria-expanded="false" aria-controls="mtl-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#mtl-bibtex" role="button" aria-expanded="false" aria-controls="mtl-bibtex">BibTeX</a>                
          
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/deeplo-2022/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2022.deeplo-1.13/" target="_blank">Paper</a>    
          <a class="badge grey waves-effect font-weight-light mr-1" href="./Posters/MTL.pdf" target="_blank">Poster</a>            
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="mtl-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Curriculum learning strategies in prior multi-task learning approaches arrange datasets in a difficulty hierarchy either based on human perception or by exhaustively searching the optimal arrangement. However, human perception of difficulty may not always correlate well with machine interpretation leading to poor performance and exhaustive search is computationally expensive. Addressing these concerns, we propose two classes of techniques to arrange training instances into a learning curriculum based on difficulty scores computed via model-based approaches. The two classes i.e Dataset-level and Instance-level differ in granularity of arrangement. Through comprehensive experiments with 12 datasets, we show that instance-level and dataset-level techniques result in strong representations as they  lead to an average performance improvement of 4.17% and 3.15% over their respective baselines. Furthermore, we find that most of this improvement comes from correctly answering the difficult instances, implying a greater efficacy of our techniques on difficult tasks.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="mtl-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
          @inproceedings{varshney-mishra-and-chitta-baral-2022-model,
    title = "Let the Model Decide its Curriculum for Multitask Learning",
    author = "Varshney, Neeraj  and
      Mishra and Chitta Baral, Swaroop",
    booktitle = "Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing",
    month = jul,
    year = "2022",
    address = "Hybrid",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deeplo-1.13",
    pages = "117--125",
    abstract = "t",
}

}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->



<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          AAAI
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">An Architecture for Novelty Handling in a Multi-Agent Stochastic Environment: Case Study in Open-World Monopoly</h5>
      <div class="author">
        <a href="https://www.tungthai155.com/" target="_blank">Tung Thai</a>, <a href="https://scholar.google.com/citations?user=njWU4SEAAAAJ&hl=en" target="_blank">Ming Shen</a>, <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.com/citations?user=So86Wl4AAAAJ&hl=en" target="_blank">Sriram Gopalakrishnan</a>, <a href="https://scholar.google.com/citations?user=3Nqzr90AAAAJ&hl=en" target="_blank">Utkarsh Soni</a>, <a href="https://scholar.google.com/citations?user=5yT3GScAAAAJ&hl=en" target="_blank">Matthias Scheutz</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>, <a href="https://scholar.google.com/citations?user=-mHoWKEAAAAJ&hl=en" target="_blank">Jivko Sinapov</a>
          
      </div>
      <div>
        <p class="periodical font-italic">
          
            AAAI Spring Symposium 2022
        
      </p>
      <p  class="periodical font">We introduce an architecture that allows agents to detect novelties, characterize those novelties, and build an appropriate adaptive model to accommodate them.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#aaaisymp-abstract" role="button" aria-expanded="false" aria-controls="aaaisymp-abstract">Abstract</a>        
        
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#aaaisymp-bibtex" role="button" aria-expanded="false" aria-controls="aaaisymp-bibtex">BibTeX</a>                
           -->
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://usc-isi-i2.github.io/AAAI2022SS/" target="_blank">Publisher</a>                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://usc-isi-i2.github.io/AAAI2022SS/papers/SSS-22_paper_50.pdf" target="_blank">arXiv</a>                
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="aaaisymp-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            The ability of AI agents and architectures to detect and adapt to sudden changes in their environments remains an outstanding challenge. In the context of multi-agent games, the agent may face novel situations where the rules of the game, the available actions, the environment dynamics, the behavior of other agents, as well as the agents goals suddenly change. In this paper, we introduce an architecture that allows agents to detect novelties, characterize those novelties, and build an appropriate adaptive model to accommodate them. Our agent utilizes logic and reasoning (specifically, Answer Set Programming) to characterize novelties into different categories, as to enable the agent to adapt to the novelty while maintaining high performance in the game. We demonstrate the effectiveness of the proposed agent architecture in a multi-agent imperfect information board game, Monopoly. We measure the success of the architecture by comparing our method to heuristics, and vanilla Monte-Carlo Tree Search approaches. Our results indicate precise novelty detection, and significant improvements in the performance of agents utilizing the novelty handling architecture.
          </div>
        </div>
      </div>
            
      <!-- <div class="col mt-2 p-0">
        <div id="wang2022benchmarking-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
          @article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}</pre>

          </div>
        </div>
      </div>
       -->

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->




<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          arXiv
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">"John is 50 years old, can his son be 65?" Evaluating NLP Models' Understanding of Feasibility</h5>
      <div class="author">
        <a href="https://scholar.google.com/citations?user=ydjuhxsAAAAJ&hl=en" target="_blank">Himanshu Gupta</a>, <nobr><em>Neeraj Varshney</em></nobr>, <a href="https://scholar.google.co.in/citations?user=-7LK2SwAAAAJ&hl=en" target="_blank">Swaroop Mishra</a>, <a href="https://kuntalkumarpal.github.io/" target="_blank">Kuntal Kumar Pal</a>, <a>Saurabh Arjun Sawant</a>, <a>Kevin Scaria</a>, <a>Siddharth Goyal</a>, <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>
          
      </div>
      <!-- <div>
        <p class="periodical font-italic">
          
            AAAI Spring Symposium 2022
        
      </p>
      <p  class="periodical font">We introduce an architecture that allows agents to detect novelties, characterize those novelties, and build an appropriate adaptive model to accommodate them.
      </div> -->
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#feasibility-abstract" role="button" aria-expanded="false" aria-controls="feasibility-abstract">Abstract</a>        
        
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#aaaisymp-bibtex" role="button" aria-expanded="false" aria-controls="aaaisymp-bibtex">BibTeX</a>                
           -->
            <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://usc-isi-i2.github.io/AAAI2022SS/" target="_blank">Publisher</a> -->                  
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2210.07471" target="_blank">arXiv</a>                
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="feasibility-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In current NLP research, large-scale language models and their abilities are widely being discussed. Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities. This work focuses on a simple commonsense ability, reasoning about when an action (or its effect) is feasible. We introduce FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility. We show that even state-of-the-art models such as GPT-3 struggle to answer the feasibility questions correctly. Specifically, on (MCQ, BCQ) questions, GPT-3 achieves accuracy of just (19%, 62%) and (25%, 64%) in zero-shot and few-shot settings, respectively. We also evaluate models by providing relevant knowledge statements required to answer the question and find that the additional knowledge leads to a 7% gain in performance, but the overall performance still remains low. These results make one wonder how much commonsense knowledge about action feasibility is encoded in GPT-3 and how well the model can reason about it.
          </div>
        </div>
      </div>
            
      <!-- <div class="col mt-2 p-0">
        <div id="wang2022benchmarking-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
          @article{wang2022benchmarking,
  title={Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}</pre>

          </div>
        </div>
      </div>
       -->

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<!-- -------- -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
    
      
        <span class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;">
          arXiv
        </span>
      
    
      </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="verma2022learninguser" class="col p-0">
      <h5 class="title mb-0">Can Transformers Reason About Effects of Actions?</h5>
      <div class="author">
          <a href="https://pratyay-banerjee.github.io/" target="_blank">Pratyay Banerjee</a>,
                <a href="https://www.public.asu.edu/~cbaral/" target="_blank">Chitta Baral</a>,
                <a href="https://luomancs.github.io/" target="_blank">Man Luo</a>,
                <a href="https://scholar.google.com/citations?user=ItxA4esAAAAJ&hl=en" target="_blank">Arindam Mitra</a>, 
                <a href="https://kuntalkumarpal.github.io/" target="_blank">Kuntal Pal</a>,
                <a href="https://www.cs.nmsu.edu/~tson/" target="_blank">Tran C. Son</a>,
                <nobr><em>Neeraj Varshney</em></nobr>
          
      </div>
      <div>
        <p class="periodical font-italic">
          arXiv          
        
      </p>
      <p  class="periodical font">Reasoning about action and change has been a top focus in the knowledge representation subfield of AI from the early days of AI and more recently it has been a highlight aspect in common sense question answering. We consider four action domains (Blocks World, Logistics, Dock-Worker-Robots and a Generic Domain) in natural language and create QA datasets that involve reasoning about the effects of actions in these domains. We investigate the ability of transformers to (a) learn to reason in these domains and (b) transfer that learning from the generic domains to the other domains.
      </div>
    
      <div class="col p-0">        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#banerjee2020can-abstract" role="button" aria-expanded="false" aria-controls="banerjee2020can-abstract">Abstract</a>        
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#banerjee2020can-bibtex" role="button" aria-expanded="false" aria-controls="banerjee2020can-bibtex">BibTeX</a>                
          
            <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2012.09938" target="_blank">Publisher</a>                  
         -->
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2012.09938" target="_blank">arXiv</a>                
      </div>          
      
      <div class="col mt-2 p-0">
        <div id="banerjee2020can-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            A recent work has shown that transformers are able to "reason" with facts and rules in a limited setting where the rules are natural language expressions of conjunctions of conditions implying a conclusion. Since this suggests that transformers may be used for reasoning with knowledge given in natural language, we do a rigorous evaluation of this with respect to a common form of knowledge and its corresponding reasoning -- the reasoning about effects of actions. Reasoning about action and change has been a top focus in the knowledge representation subfield of AI from the early days of AI and more recently it has been a highlight aspect in common sense question answering. We consider four action domains (Blocks World, Logistics, Dock-Worker-Robots and a Generic Domain) in natural language and create QA datasets that involve reasoning about the effects of actions in these domains. We investigate the ability of transformers to (a) learn to reason in these domains and (b) transfer that learning from the generic domains to the other domains.
          </div>
        </div>
      </div>
            
      <div class="col mt-2 p-0">
        <div id="banerjee2020can-bibtex" class="collapse">
          <div class="bibtex card card-body font-weight-light mr-0 mr-sm-3 p-3">
            <pre>
          @article{banerjee2020can,
  title={Can Transformers Reason About Effects of Actions?},
  author={Banerjee, Pratyay and Baral, Chitta and Luo, Man and Mitra, Arindam and Pal, Kuntal and Son, Tran C and Varshney, Neeraj},
  journal={arXiv preprint arXiv:2012.09938},
  year={2020}
}</pre>

          </div>
        </div>
      </div>
      

      
      
    </div>

  </div>
  
</div>


</li>

</ol>
    </div>
    <div class="col-sm-1 align-self-start mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
  </div>

<!-- -------- -->



  
  <!-- Core JavaScript Files -->
  <script src="../assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="../assets/js/popper.min.js" type="text/javascript"></script>
  <script src="../assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="../assets/js/common.js"></script>

  <!-- GitHub Stars -->
  <script src="../assets/js/github-stars.js"></script>
  <script type="text/javascript">
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  </script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="../assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
